---
title: 'Scrape Webpage'
openapi: 'POST /upload/scrape_webpage'
---

import { TableOfContents } from '/snippets/table-of-contents.jsx'
import TryIt from '/snippets/try-it.mdx'

<Panel>
  <TableOfContents />
</Panel>
<TryIt />
### Examples

<Tabs>
  <Tab title="API Request">
    ```bash
    curl --request POST \
      --url 'https://api.usecortex.ai/upload/scrape_webpage?web_url=https%3A%2F%2Fwww.usecortex.ai%2F&tenant_id=tenant_1234&sub_tenant_id=sub_tenant_4567&file_id=CortexDoc1234' \
      --header 'Authorization: Bearer YOUR_API_KEY' \
      --header 'Content-Type: application/x-www-form-urlencoded'
      ```
  </Tab>
  <Tab title="TypeScript">
    ```ts
    const result = await client.upload.scrapeWebpage({
      web_url: "https://www.usecortex.ai/",
      tenant_id: "tenant_1234",
      sub_tenant_id: "sub_tenant_4567",
      file_id: "CortexDoc1234"
    });
    ```
  </Tab>
  <Tab title="Python (Sync)">
    ```python
    # Async usage is similar, just use async_client and await
    result = client.upload.scrape_webpage(
        web_url="https://www.usecortex.ai/",
        tenant_id="tenant_1234",
        sub_tenant_id="sub_tenant_4567",
        file_id="CortexDoc1234"
    )
    ```
  </Tab>
</Tabs>

Scrape and process webpage content directly from a URL. The webpage will be scraped, processed, chunked, and indexed for search and retrieval.

## Webpage Scraping & Processing Pipeline

When you scrape a webpage, it goes through a specialized processing pipeline designed for web content:

### 1. **Immediate URL Processing & Queue**
- Your webpage URL is immediately accepted and validated
- The URL is added to our scraping queue for background processing
- You receive a confirmation response with a `file_id` for tracking

### 2. **Web Scraping Phase**
Our system automatically handles:
- **URL Validation**: Ensuring the URL is accessible and valid
- **Content Scraping**: Extracting text, HTML, and metadata from the webpage
- **Structure Analysis**: Understanding page layout, headers, and content hierarchy
- **Link Extraction**: Identifying and processing internal and external links

### 3. **Content Processing & Cleaning**
- **HTML Parsing**: Converting HTML to clean, structured text
- **Content Filtering**: Removing navigation, ads, and irrelevant content
- **Text Normalization**: Cleaning and standardizing web content
- **Language Detection**: Identifying the webpage's language for optimal processing

### 4. **Intelligent Chunking**
- Web content is split into semantically meaningful chunks
- Chunk size is optimized for both context preservation and search accuracy
- Overlapping boundaries ensure no information is lost between chunks
- Metadata is preserved and associated with each chunk

### 5. **Embedding Generation**
- Each chunk is converted into high-dimensional vector embeddings
- Embeddings capture semantic meaning and context
- Vectors are optimized for similarity search and retrieval

### 6. **Indexing & Database Updates**
- Embeddings are stored in our vector database for fast similarity search
- Full-text search indexes are created for keyword-based queries
- Metadata is indexed for filtering and faceted search
- Cross-references are established for related web content

### 7. **Quality Assurance**
- Automated quality checks ensure scraping accuracy
- Content validation verifies extracted text completeness
- Embedding quality is assessed for optimal retrieval performance

<Note>
**Processing Time**: Webpage scraping and processing typically takes 2-5 minutes. Complex pages with heavy content may take up to 10 minutes. You can check processing status using the document ID returned in the response.
</Note>

<Note>
**Default Sub-Tenant Behavior**: If you don't specify a `sub_tenant_id`, the webpage content will be uploaded to the default sub-tenant created when your tenant was set up. This is perfect for organization-wide web content that should be accessible across all departments.
</Note>

> **File ID Management**: When you provide a `file_id` as a key in the `document_metadata` object, that specific ID will be used to identify your content. If no `file_id` is provided in the `document_metadata`, the system will automatically generate a unique identifier for you. This allows you to maintain consistent references to your content across your application while ensuring every piece of content has a unique identifier.


## Error Responses

All endpoints return consistent error responses following the standard format. For detailed error information, see our [Error Responses](/api-reference/error-responses) documentation.

